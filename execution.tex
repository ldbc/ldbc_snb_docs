\section{Execution rules}\label{section:rules}

%\subsection{Execution Steps}
%\alert{Some items have to be reviewed}
%A benchmark execution is divided into the following steps:
%\begin{itemize}
%    \item \textbf{Data Preparation.} This includes running the data generator, placing
%        the generated files in a staging area, configuring storage, setting up
%        the SUT configuration and preparing any data partitions in the SUT.
%        This may include pre-allocating database space but may not include
%        loading any data or defining any schema having to do with the
%        benchmark.
%    \item \textbf{Bulk Load.} This includes defining the database schema, if any,
%        loading the initial database population, making this durably stored,
%        gathering any optimizer statistics,.   The bulk load time is reported
%        and is equal to the amount of elapsed wall clock time between starting
%        the schema definition and receiving the confirmation message of the end
%        of statistics gathering.
%    \item \textbf{Benchmark Run.} The run begins after the bulk load or after another
%        benchmark run.  If the run does not directly follow the bulk load, it
%        must start at a point in the update stream that has not previously been
%        played into the database.  In other words, a run may only include
%        update events whose timestamp is later than the latest post creation
%        date in the database prior to start of run.  The run starts when the
%        first of the test drivers sends its first message to the SUT.  If the
%        SUT is in-process with the driver the window starts when the driver
%        starts.
%    \item \textbf{Measurement Window.} The measurement window is the timed portion of
%        the benchmark run. It may begin at any time during the run.  The
%        activity during the measurement window must meet the criteria described in
%        \alert{Section XX}. The measurement window is terminated at
%        the discretion of the test sponsor at any time when the Minimum
%        Measurement Window criteria are met. All the processes constituting
%        the SUT are to be killed at the end of the window or alternatively all
%        the hardware components of the SUT are to be powered off.
%    \item \textbf{Recovery Test.} The SUT is to be restarted after the measurement
%        window and the auditor will verify that the SUT contains the entirety
%        of the last update recorded by the test driver(s) as successfully
%        committed.
%\end{itemize}
%
%\subsection{Rules for the Data Schema}
%
%\ldbcsnb may be implemented with different data models, \eg relational, RDF and
%different graph data models.  The reference schema is provided as RDFS and SQL.
%The data generator produces TTL syntax for RDF and comma separated values for
%other data models. A single attribute has a single data type. The following
%requirements apply:
%
%\begin{itemize}
%    \item \textbf{Identifier:} This is an integer value foreign key or a URI in
%        RDF.  If this is an integer column, the implementation data type should
%        support at least $2^{55}$ distinct values
%    \item \textbf{Datetime:} Should support a date range from 0000 to 9999 in
%        the year field, with a resolution of no less than one second.
%    \item \textbf{String:} A string column for names may have a variable length
%        and may have a declared maximum length, \eg 40 characters.
%    \item \textbf{Long String:} For example a post content may be a long string
%        that is often short in the data but may not declare a maximum length
%        and must support data sizes of up to 1MB \alert{Current generator max post length is set to 2000 characters}..
%\end{itemize}
%
%A single attribute in the reference schema may not be divided into multiple
%attributes in the target schema.
%
%
%A schema on the DBMS is optional.  An RDF implementation for example may work
%without one.  An RDF implementation is allowed to load the RDF reference schema
%and to take advantage of the data type and cardinality statements therein.
%
%
%A relational or graph schema may specify system specific options affecting
%storage layout.  These may for example specify vertical partitioning.  Vertical
%partitioning means anything from a column store layout with per-column
%allocated storage space to use of explicit column groups.  Any mix of row or
%column-wise storage structures is allowed as long as this is declaratively
%specified data structure by data structure.  Data structure here means for
%example table or index.
%
%
%Covering indices and clustered indices are allowed.
%If these are defined, then all replications of data implied by these must be
%maintained statement by statement, \ie each auxiliary data structure must be
%consistent with any other data structures of the table after each data
%manipulation operation.
%
%
%A covering index is an index which materializes a
%specific order of a specific subset or possibly all columns of a table.  A
%clustered index is an index which materializes all columns of a table in a
%specific order, which order may or may not be that of the primary key of the
%table. A clustered or covering index may be the primary or only representation
%of a table.
%
%
%Any subset of the columns on a covering or clustered index may be
%used for ordering the data.  A hash based index or a combination of a hash
%based and tree based index are all allowed, in row or column-wise or hybrid
%forms.
%
%
%\subsection{Rules for Implementing the Workload}
%\subsubsection{Queries' Implementation}
%
%The queries and updates may be implemented in a declarative query language or
%as procedural code using an API.  If a declarative query language is used, \eg
%SPARQL or SQL, then explicit query plans are prohibited in all the read-only
%queries. The update transactions may still consist of multiple statements,
%effectively amounting to explicit plans.
%
%
%Explicit query plans include but \alert{are not limited to (this is too ambiguous and dangerous)}:
%\begin{itemize}
%    \item Directives or hints specifying a join order or join type
%    \item Directives or hints specifying an access path, \eg which index to use
%    \item Directive or hints specifying an expected cardinality, selectivity,
%        fanout or any other information that pertains to the expected number or
%        results or cost of all or part of the query.
%\end{itemize}
%
%\subsubsection{Auxiliary Data Structures and Pre-computation}
%Auxiliary data structures and pre-computations are allowed.  A pre-computation
%may be implemented as client side logic in the test driver, as stored
%procedures or as triggers.  In all cases the operations, whether one or many,
%must constitute a single transaction.  A SPARQL protocol operation consisting
%of multiple statements may be a valid implementation of the if the SUT executes
%the statements as a single transaction. Other pre-computation of query results
%is explicitly prohibited.
%
%\subsubsection{ACID Compliance}
%
%The interactive workload requires full ACID support from the SUT.
%\begin{itemize}
%    \item \textbf{Atomicity.} All the updates in a transaction must either take
%        place or be all cancelled.
%    \item \textbf{Consistency.} If a database object, \eg table, has auxiliary
%        data structures, \eg indices, the content of these must be consistent
%        after the commit or rollback of a transaction.   If multiple client
%        application threads share one transaction context, these may
%        transiently see inconsistent states, \eg there may be a time when an
%       insert of a row is reflected in one index of a table but not in
%        another.
%    \item \textbf{Isolation.} If a transaction reads the database with intent
%        to update, the DBMS must guarantee that repeating the same read within
%        the same transaction will return the same data.  This also means that
%        no more and no less data rows must be returned.  In other words, this
%        corresponds to snapshot or to serializable isolation.  This level of
%        isolation is applied for the operations where the transaction mix so
%        specifies.  If the database is accessed without transaction context or
%        without intent too update, then the DBMS should provide read committed
%        semantics, \eg repeating the same read may produce different results
%        but these results may never include effects of pending uncommitted
%        transactions.
%    \item \textbf{Durability.} The effects of a transaction must be made
%        durable against instantaneous failure before the SUT confirms the
%        successful commit of a transaction to the application. For systems
%        using a transaction log, this implies syncing the durable media of the
%        transaction log before confirming success to the application.  This
%        will typically entail group commit where transactions that fall in the
%        same short window are logged together and the logging device will
%        typically be an SSD or battery backed RAM on a storage controller.  For
%        systems using replication for durability, this will entail receipt of a
%        confirmation message from the replicating party before confirming
%        successful commit to the application.
%\end{itemize}
%
%\subsection{Rules for Running the Test Driver}
%
%A qualifying run must use the \ldbcsnb test driver provided. The test driver
%implements the workload described in \alert{Section XX}.
%The test driver may be modified by the test sponsor for purposes of
%interfacing to the SUT. The parameter generation and result recording and
%workload scheduling parts of the test driver cannot be changed.  The
%auditor needs to have access to the test driver source code used for producing
%the driver used in the reported run.
%
%\alert{ Check whether this is out of date or not}
%
%The test driver is scale-out capable.  Many
%instances of the test driver may be used in a test run.   The number and
%configuration of the test drivers must be disclosed, along with hardware
%details of the platform running the driver(s), together with details of the
%network interface connecting the drivers too the SUT.  The SUT hardware may
%also be used for hosting the driver(s), at the discretion of the test sponsor.
%
%
%A separate test summary tool provided with the test driver analyzes the test
%driver log(s) after a measurement window is completed.  The tool produces for
%each of the distinct queries and transactions the following summary:
%\begin{itemize}
%    \item Count of executions
%    \item Minimum/average/90th percentile/maximum execution time.
%    \item Start and end date of the window in real time and in simulation time.
%    \item Metric in operations per second at scale. (ops) (throughout rating)
%    \item Number of test drivers
%    \item Number of database sessions (threads) per test driver
%\end{itemize}
%
%
%\subsection{Rules for Scaling}
%\ldbcsnb provides predefined scale factors, as described in \alert{Section XX}.
%The validation scale factor is 1.  Official \ldbcsnb results may be published at
%any of the provided scale factors.
%
%
%\subsection{Rules for Checkpointing}
%A checkpoint is defined as the operation which causes data persisted in a
%transaction log to become durable outside of the transaction log. In specific,
%this means that a SUT restart after instantaneous failure following the
%completion of the checkpoint may not have recourse to transaction log entries
%written before the end of the checkpoint.
%
%
%A checkpoint typically involves a
%synchronization barrier at which all data committed prior too the moment is
%required to be in durable storage that does not depend on the transaction log.
%
%
%Not all DBMSs use a checkpointing mechanism for durability. For example a
%system may rely on redundant storage of data for durability guarantees against
%instantaneous failure of a single server.
%
%
%The measurement window may contain a
%checkpoint. If the measurement window does not contain one, then the restart
%test will involve redoing all the updates in the window as part of the recovery
%test.
